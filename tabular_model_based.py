# -*- coding: utf-8 -*-
"""tabular_model_based.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10n-5DA9XmCtHJNFonN548pRYkVxUNPBM
"""

import numpy as np


def policy_evaluation(env, policy, gamma, theta, max_iterations):
    # initialise the value function to zeros for each state
    value = np.zeros(env.n_states, dtype=np.float)

    for num in range(max_iterations):
        stop_criterion = 0  # initialise the stop criteria

        for state in range(env.n_states):
            # value of current state
            current_value = value[state]
            expected_discounted_return = 0

            for next_state in range(env.n_states):
                # transition probability from current to next_state
                transition_prob = env.p(next_state, state, policy[state])
                # expected reward from current state to next state plus discounted value of next state
                discounted_reward = env.r(next_state, state, policy[state]) + (gamma * value[next_state])
                # calculate the expected return by summation
                expected_discounted_return += (transition_prob * discounted_reward)

            # update value[state] with the updated expected discounted return
            value[state] = expected_discounted_return
            # absolute difference between current value and next_state
            delta = abs(current_value - value[state])
            # stop_criterion = max(absolute maximum change in the value, current stop_criterion)
            stop_criterion = max(delta, stop_criterion)

        # Check stop criterion to determine if break or not
        if stop_criterion < theta:
            break

    return value


def policy_improvement(env, value, gamma):
    policy = np.zeros(env.n_states, dtype=int)

    # boolean to keep track of improvement in policy
    policy_stable = True

    for state in range(env.n_states):
        current_policy = policy[state]
        actions = env.a(state)  # get all actions -- defined the environment

        # store the action indices
        action_list = []
        for act in range(len(actions)):
            action_list.append(act)  # store each action index in new_actions
        # calculate the new policy[state] by taking the max value for all actions
        policy[state] = action_list[int(np.argmax([sum([env.p(next_state, state, action) *
                                                        (env.r(next_state, state, action) + gamma * value[next_state])
                                                        for next_state in range(env.n_states)]) for action in
                                                   action_list]))]

        # if policy has changed then policy_stable is False
        if current_policy != policy[state]:
            policy_stable = False

    return policy, policy_stable


def policy_iteration(env, gamma, theta, max_iterations, policy=None):
    value = np.zeros(env.n_states, dtype=int)  # initialise value with dimension equaling the number of states.
    if policy is None:
        policy = np.zeros(env.n_states, dtype=int)
    else:
        policy = np.array(policy, dtype=int)

    num_iterations = 0
    for num in range(max_iterations):
        current_policy = policy #set current policy to policy
        value = policy_evaluation(env, policy, gamma, theta, max_iterations) #call the policy_evaluation function
        policy, policy_stable = policy_improvement(env, value, gamma) #call the policy_improvement
        num_iterations = num_iterations + 1
        #stop criteria for when the policy converges and there is no change in the estimates i.e policy_ i = policy_i+1
        if np.array_equal(policy, current_policy):
            break
    print(f"Number of iterations required in policy iteration is: {num_iterations}")
    return policy, value


def value_iteration(env, gamma, theta, max_iterations, value=None):
    if value is None:
        value = np.zeros(env.n_states)
    else:
        value = np.array(value, dtype=np.float)
    num_iterations = 0
    for num in range(max_iterations):
        stop_criterion = 0
        for state in range(env.n_states):
            current_value = value[state]
            value_list = []  # an array to store the values
            actions = env.a(state)
            for action in range(len(actions)):

                expected_discounted_return = 0
                for next_state in range(env.n_states):
                    # calculate the transition probability and discounted reward
                    transition_prob = env.p(next_state, state, action)
                    discounted_reward = env.r(next_state, state, action) + (gamma * value[next_state])

                    expected_discounted_return += (transition_prob * discounted_reward)

                value_list.append(expected_discounted_return)

            # Choose the max value from the new_value list
            value[state] = max(value_list)
            # absolute difference between current value and next_state
            delta = np.abs(current_value - value[state])
            # stop_criterion = max(absolute maximum change in the value, current stop_criterion)
            stop_criterion = max(delta, stop_criterion)

        # Check stop criterion to determine if break or not
        if stop_criterion < theta:
            break
        num_iterations += 1

    policy = np.zeros(env.n_states, dtype=int)  # initialise policy
    for state in range(env.n_states):
        action_list = []
        action_values = []
        actions = env.a(state)
        for action in range(len(actions)):
            for next_state in range(env.n_states):
                # transition probability and discounted reward for next_state
                transition_prob = env.p(next_state, state, action)
                discounted_reward = env.r(next_state, state, action) + (gamma * value[next_state])
                # append action_index and corresponding value
                action_list.append(action)
                action_values.append(transition_prob * discounted_reward)
            # pick the action the returns the max value
        optimal_action = action_list[action_values.index(max(action_values))]
        policy[state] = optimal_action

    print(f"Number of iterations required in value iteration: {num_iterations}")
    return policy, value
